{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jianzhiw/spaCy/blob/master/spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfHfVS5TiHZB",
        "colab_type": "text"
      },
      "source": [
        "# [Spacy](https://spacy.io/) #\n",
        "\n",
        "spaCy is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.\n",
        "<br></br>\n",
        "\n",
        "Future work: Explore [NLTK](https://www.nltk.org/), a similar library for language processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wRnznSLbZ2G",
        "colab_type": "code",
        "outputId": "447b73f5-5c16-4111-8a16-d1bcee53c9fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "# Install spaCy\n",
        "!pip install spacy\n",
        "\n",
        "# Download large English model for spaCy\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "# Install textacy which will also be useful\n",
        "!pip install textacy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: textacy in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (3.1.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.3)\n",
            "Requirement already satisfied: scikit-learn<0.21.0,>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.20.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.16.4)\n",
            "Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
            "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.9.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.3.0)\n",
            "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.0.7)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.28.1)\n",
            "Requirement already satisfied: spacy>=2.0.12 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.1.6)\n",
            "Requirement already satisfied: jellyfish<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.6.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.21.0)\n",
            "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.11->textacy) (4.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.2.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (7.0.8)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.12->textacy) (0.9.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djGQMoctggcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bttm8tDyccd7",
        "colab_type": "code",
        "outputId": "57be7d2e-1501-4667-8e36-b5adc5bb4612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Load the large English NLP model\n",
        "# If you have error here, go to runtime -> restart runtime and rerun\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# The text we want to examine\n",
        "text = \"\"\"London is the capital and most populous city of England and \n",
        "the United Kingdom.  Standing on the River Thames in the south east \n",
        "of the island of Great Britain, London has been a major settlement \n",
        "for two millennia. It was founded by the Romans, who named it Londinium.\n",
        "\"\"\"\n",
        "\n",
        "# Parse the text with spaCy. This runs the entire pipeline.\n",
        "doc = nlp(text)\n",
        "\n",
        "# 'doc' now contains a parsed version of text. We can use it to do anything we want!\n",
        "# For example, this will print out all the named entities that were detected:\n",
        "for entity in doc.ents:\n",
        "    print(f\"{entity.text} ({entity.label_})\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "London (GPE)\n",
            "England (GPE)\n",
            "the United Kingdom (GPE)\n",
            "the River Thames (LOC)\n",
            "Great Britain (GPE)\n",
            "London (GPE)\n",
            "two millennia (DATE)\n",
            "Romans (NORP)\n",
            "Londinium (LOC)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lINb7E6Ih-QD",
        "colab_type": "text"
      },
      "source": [
        "View the [annotations](https://spacy.io/api/annotation#named-entities) here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNrNBV2dit_x",
        "colab_type": "text"
      },
      "source": [
        "# Remove the detected name #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TzWd_QiidvZ",
        "colab_type": "code",
        "outputId": "14691796-2e6b-4f90-e8df-a6c6d1acebd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Replace a token with \"REDACTED\" if it is a name\n",
        "def replace_name_with_placeholder(token):\n",
        "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
        "        return \"[REDACTED] \"\n",
        "    else:\n",
        "        return token.string\n",
        "\n",
        "# Loop through all the entities in a document and check if they are names\n",
        "def scrub(text):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        ent.merge()\n",
        "    tokens = map(replace_name_with_placeholder, doc)\n",
        "    return \"\".join(tokens)\n",
        "\n",
        "s = \"\"\"\n",
        "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky’s \n",
        "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
        "\"\"\"\n",
        "\n",
        "print(scrub(s))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "In 1950, [REDACTED] published his famous article \"Computing Machinery and Intelligence\". In 1957, [REDACTED] \n",
            "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjFhiMIQizoZ",
        "colab_type": "text"
      },
      "source": [
        "# Extracting Facts #\n",
        "\n",
        "What you can do with spaCy right out of the box is pretty amazing. But you can also use the parsed output from spaCy as the input to more complex data extraction algorithms. There’s a python library called textacy that implements several common data extraction algorithms on top of spaCy. It’s a great starting point.\n",
        "<br></br>\n",
        "\n",
        "One of the algorithms it implements is called [Semi-structured Statement Extraction](https://chartbeat-labs.github.io/textacy/api_reference/information_extraction.html?highlight=semistructured#textacy.extract.semistructured_statements). We can use it to search the parse tree for simple statements where the subject is “London” and the verb is a form of “be”. That should help us find facts about London."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKnc_Dq8kqD5",
        "colab_type": "code",
        "outputId": "eb63897a-48de-407f-b584-9a52f20e1d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import textacy.extract\n",
        "\n",
        "# The text we want to examine\n",
        "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
        "Standing on the River Thames in the south east of the island of Great Britain, \n",
        "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
        "who named it Londinium.\n",
        "\"\"\"\n",
        "\n",
        "# Parse the document with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract semi-structured statements\n",
        "statements = textacy.extract.semistructured_statements(doc, \"London\")\n",
        "\n",
        "# Print the results\n",
        "print(\"Here are the things I know about London:\")\n",
        "\n",
        "for statement in statements:\n",
        "    subject, verb, fact = statement\n",
        "    print(f\" - {fact}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here are the things I know about London:\n",
            " - the capital and most populous city of England and  the United Kingdom.  \n",
            "\n",
            " - a major settlement  for two millennia.  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvg4sVrxoKbG",
        "colab_type": "text"
      },
      "source": [
        "For extra credit, try installing the [neuralcoref](https://github.com/huggingface/neuralcoref?source=post_page---------------------------) library (I was having some problem in using the library above in Colab) and adding Coreference Resolution to your pipeline. That will get you a few more facts since it will catch sentences that talk about “it” instead of mentioning “London” directly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJDB2Smzb0Ml",
        "colab_type": "text"
      },
      "source": [
        "[Source](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbITp1jr9DRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}